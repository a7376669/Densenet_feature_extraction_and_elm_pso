{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa292d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbf89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, ZeroPadding2D\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import log_loss\n",
    "from scale_layer import Scale\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import tensorflow\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import seaborn as sns\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def densenet121_model(img_rows, img_cols, color_type=1, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.5, dropout_rate=0.0, weight_decay=1e-4, num_classes=None):\n",
    "    '''\n",
    "    DenseNet 121 Model for Keras\n",
    "\n",
    "    Model Schema is based on \n",
    "    https://github.com/flyyufelix/DenseNet-Keras\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global concat_axis\n",
    "    img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
    "    concat_axis = 3\n",
    "\n",
    "\n",
    "    # From architecture for ImageNet (Table 1 in the paper)\n",
    "    nb_filter = 64\n",
    "    nb_layers = [6,12,24,16] # For DenseNet-121\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(nb_filter, (7,7), strides=(2, 2), name='conv1', use_bias=False)(img_input)#109 109 64    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        stage = block_idx+2\n",
    "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "        # Add transition_block\n",
    "        x = transition_block(x, stage, nb_filter, dropout_rate=dropout_rate)\n",
    "        nb_filter = int(nb_filter)\n",
    "\n",
    "    final_stage = stage + 1\n",
    "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis )(x)\n",
    "    x = Activation('relu')(x)\n",
    "    print(x.type)\n",
    "\n",
    "    x_fc = GlobalAveragePooling2D()(x)\n",
    "    x_fc = Dense(1000)(x_fc)\n",
    "    x_fc = Activation('softmax')(x_fc)\n",
    "    \n",
    "    model = Model(img_input, x_fc)\n",
    "\n",
    "    # The method below works since pre-trained weights are stored in layers but not in the model\n",
    "    x_newfc = GlobalAveragePooling2D()(x)\n",
    "    x_newfc = Dense(num_classes)(x_newfc)\n",
    "    x_newfc = Activation('softmax')(x_newfc)\n",
    "\n",
    "    model = Model(img_input, x_newfc)\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b21edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(x, stage, branch, nb_filter, dropout_rate=None):\n",
    "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout '''\n",
    "\n",
    "\n",
    "    # 1x1 Convolution (Bottleneck layer)\n",
    "    inter_channel = nb_filter * 4  \n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(inter_channel, (1, 1), use_bias=False)(x)\n",
    "    y=x\n",
    "    #print(y.shape)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 3x3 Convolution\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "    z=x\n",
    "    #print(z.shape)\n",
    "    x = Convolution2D(nb_filter, (3, 3), use_bias=False)(x)\n",
    "    w=x\n",
    "    #print(w.shape)\n",
    "    #print(type(w))\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, stage, nb_filter, dropout_rate=None):\n",
    "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout  '''\n",
    "\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(nb_filter), 1, 1, use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, grow_nb_filters=True):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_layers: the number of layers of conv_block to append to the model.\n",
    "            nb_filter: number of filters\n",
    "            growth_rate: growth rate\n",
    "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "    '''\n",
    "\n",
    "    concat_feat = x\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        branch = i+1\n",
    "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate)\n",
    "        #concat_feat = Concatenate(axis=concat_axis)([concat_feat, x])\n",
    "        my_concat = Lambda (lambda x: K.concatenate([x[0],x[1]],axis=concat_axis))\n",
    "        concat_feat=my_concat([concat_feat, x])\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    return concat_feat, nb_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2de96f",
   "metadata": {},
   "source": [
    "# Implement your own load_data() module for your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3bba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chickenpox']\n",
      "(420, 224, 224, 3) (140, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[0.3372549 , 0.6039216 , 0.6313726 ],\n",
       "          [0.3372549 , 0.6039216 , 0.6313726 ],\n",
       "          [0.3372549 , 0.6039216 , 0.6313726 ],\n",
       "          ...,\n",
       "          [0.39215687, 0.6431373 , 0.6862745 ],\n",
       "          [0.38431373, 0.63529414, 0.6784314 ],\n",
       "          [0.38039216, 0.6313726 , 0.6745098 ]],\n",
       " \n",
       "         [[0.34901962, 0.6156863 , 0.6431373 ],\n",
       "          [0.34509805, 0.6117647 , 0.6392157 ],\n",
       "          [0.34509805, 0.6117647 , 0.6392157 ],\n",
       "          ...,\n",
       "          [0.4       , 0.6509804 , 0.69411767],\n",
       "          [0.39215687, 0.6431373 , 0.6862745 ],\n",
       "          [0.38431373, 0.63529414, 0.6784314 ]],\n",
       " \n",
       "         [[0.34901962, 0.6156863 , 0.6431373 ],\n",
       "          [0.34901962, 0.6156863 , 0.6431373 ],\n",
       "          [0.34901962, 0.6156863 , 0.6431373 ],\n",
       "          ...,\n",
       "          [0.40784314, 0.65882355, 0.7019608 ],\n",
       "          [0.4       , 0.6509804 , 0.69411767],\n",
       "          [0.39215687, 0.6431373 , 0.6862745 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.5019608 , 0.7607843 , 0.75686276],\n",
       "          [0.49019608, 0.7490196 , 0.74509805],\n",
       "          [0.47058824, 0.7294118 , 0.7254902 ],\n",
       "          ...,\n",
       "          [0.57254905, 0.7882353 , 0.8       ],\n",
       "          [0.5686275 , 0.78431374, 0.79607844],\n",
       "          [0.5686275 , 0.78431374, 0.79607844]],\n",
       " \n",
       "         [[0.49803922, 0.75686276, 0.7529412 ],\n",
       "          [0.49019608, 0.7490196 , 0.74509805],\n",
       "          [0.48235294, 0.7411765 , 0.7372549 ],\n",
       "          ...,\n",
       "          [0.57254905, 0.7882353 , 0.8       ],\n",
       "          [0.5686275 , 0.78431374, 0.79607844],\n",
       "          [0.5686275 , 0.78431374, 0.79607844]],\n",
       " \n",
       "         [[0.4627451 , 0.72156864, 0.7176471 ],\n",
       "          [0.46666667, 0.7254902 , 0.72156864],\n",
       "          [0.4745098 , 0.73333335, 0.7294118 ],\n",
       "          ...,\n",
       "          [0.57254905, 0.7882353 , 0.8       ],\n",
       "          [0.5686275 , 0.78431374, 0.79607844],\n",
       "          [0.5686275 , 0.78431374, 0.79607844]]],\n",
       " \n",
       " \n",
       "        [[[0.75686276, 0.8627451 , 0.99607843],\n",
       "          [0.72156864, 0.827451  , 0.9607843 ],\n",
       "          [0.7058824 , 0.8117647 , 0.94509804],\n",
       "          ...,\n",
       "          [0.9843137 , 1.        , 0.99607843],\n",
       "          [0.99215686, 1.        , 0.99607843],\n",
       "          [0.9882353 , 1.        , 0.99215686]],\n",
       " \n",
       "         [[0.77254903, 0.8784314 , 1.        ],\n",
       "          [0.7411765 , 0.84705883, 0.98039216],\n",
       "          [0.7254902 , 0.83137256, 0.9647059 ],\n",
       "          ...,\n",
       "          [0.98039216, 1.        , 0.99215686],\n",
       "          [0.99215686, 1.        , 0.99607843],\n",
       "          [0.99215686, 1.        , 0.99607843]],\n",
       " \n",
       "         [[0.78039217, 0.8862745 , 1.        ],\n",
       "          [0.75686276, 0.8627451 , 0.99607843],\n",
       "          [0.7490196 , 0.85490197, 0.9882353 ],\n",
       "          ...,\n",
       "          [0.96862745, 0.9882353 , 0.98039216],\n",
       "          [0.98039216, 1.        , 0.99215686],\n",
       "          [0.9843137 , 1.        , 0.99607843]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.39215687, 0.38431373, 0.5647059 ],\n",
       "          [0.37254903, 0.3647059 , 0.54509807],\n",
       "          [0.3764706 , 0.36862746, 0.54901963],\n",
       "          ...,\n",
       "          [0.32156864, 0.29803923, 0.41960785],\n",
       "          [0.37254903, 0.34901962, 0.47058824],\n",
       "          [0.37254903, 0.34901962, 0.47058824]],\n",
       " \n",
       "         [[0.39215687, 0.38039216, 0.56078434],\n",
       "          [0.37254903, 0.36078432, 0.5411765 ],\n",
       "          [0.38039216, 0.36862746, 0.54901963],\n",
       "          ...,\n",
       "          [0.33333334, 0.30980393, 0.43137255],\n",
       "          [0.38431373, 0.36078432, 0.48235294],\n",
       "          [0.37254903, 0.34901962, 0.47058824]],\n",
       " \n",
       "         [[0.3647059 , 0.3529412 , 0.53333336],\n",
       "          [0.34901962, 0.3372549 , 0.5176471 ],\n",
       "          [0.36078432, 0.34901962, 0.5294118 ],\n",
       "          ...,\n",
       "          [0.34901962, 0.3254902 , 0.44705883],\n",
       "          [0.3882353 , 0.3647059 , 0.4862745 ],\n",
       "          [0.36078432, 0.3372549 , 0.45882353]]],\n",
       " \n",
       " \n",
       "        [[[0.58431375, 0.6666667 , 0.8666667 ],\n",
       "          [0.6117647 , 0.69411767, 0.89411765],\n",
       "          [0.5882353 , 0.67058825, 0.87058824],\n",
       "          ...,\n",
       "          [0.6313726 , 0.7137255 , 0.91764706],\n",
       "          [0.62352943, 0.70980394, 0.9137255 ],\n",
       "          [0.62352943, 0.70980394, 0.9137255 ]],\n",
       " \n",
       "         [[0.5882353 , 0.67058825, 0.87058824],\n",
       "          [0.6117647 , 0.69411767, 0.89411765],\n",
       "          [0.5882353 , 0.67058825, 0.87058824],\n",
       "          ...,\n",
       "          [0.627451  , 0.70980394, 0.9137255 ],\n",
       "          [0.62352943, 0.70980394, 0.9137255 ],\n",
       "          [0.62352943, 0.70980394, 0.9137255 ]],\n",
       " \n",
       "         [[0.5882353 , 0.67058825, 0.87058824],\n",
       "          [0.6117647 , 0.69411767, 0.89411765],\n",
       "          [0.5921569 , 0.6745098 , 0.8745098 ],\n",
       "          ...,\n",
       "          [0.627451  , 0.70980394, 0.9137255 ],\n",
       "          [0.62352943, 0.70980394, 0.9137255 ],\n",
       "          [0.62352943, 0.70980394, 0.9137255 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.05882353, 0.11372549, 0.22745098],\n",
       "          [0.05490196, 0.10980392, 0.22352941],\n",
       "          [0.09411765, 0.14509805, 0.27058825],\n",
       "          ...,\n",
       "          [0.81960785, 0.8666667 , 0.9607843 ],\n",
       "          [0.81960785, 0.8666667 , 0.9607843 ],\n",
       "          [0.81960785, 0.8666667 , 0.9607843 ]],\n",
       " \n",
       "         [[0.07843138, 0.13333334, 0.24705882],\n",
       "          [0.03137255, 0.08627451, 0.2       ],\n",
       "          [0.05490196, 0.11372549, 0.23921569],\n",
       "          ...,\n",
       "          [0.827451  , 0.8627451 , 0.96862745],\n",
       "          [0.827451  , 0.8627451 , 0.96862745],\n",
       "          [0.827451  , 0.8627451 , 0.96862745]],\n",
       " \n",
       "         [[0.14509805, 0.2       , 0.3137255 ],\n",
       "          [0.05098039, 0.10588235, 0.21960784],\n",
       "          [0.03921569, 0.09803922, 0.22352941],\n",
       "          ...,\n",
       "          [0.827451  , 0.8627451 , 0.96862745],\n",
       "          [0.827451  , 0.8627451 , 0.96862745],\n",
       "          [0.827451  , 0.8627451 , 0.96862745]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.32941177, 0.43529412, 0.7490196 ],\n",
       "          [0.32941177, 0.43529412, 0.7490196 ],\n",
       "          [0.33333334, 0.4392157 , 0.7529412 ],\n",
       "          ...,\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ],\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ],\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ]],\n",
       " \n",
       "         [[0.32941177, 0.43529412, 0.7490196 ],\n",
       "          [0.33333334, 0.4392157 , 0.7529412 ],\n",
       "          [0.33333334, 0.4392157 , 0.7529412 ],\n",
       "          ...,\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ],\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ],\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ]],\n",
       " \n",
       "         [[0.32941177, 0.43529412, 0.7490196 ],\n",
       "          [0.33333334, 0.4392157 , 0.7529412 ],\n",
       "          [0.33333334, 0.4392157 , 0.7529412 ],\n",
       "          ...,\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ],\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ],\n",
       "          [0.8509804 , 0.9647059 , 0.8627451 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.44705883, 0.64705884, 0.8156863 ],\n",
       "          [0.44705883, 0.64705884, 0.8156863 ],\n",
       "          [0.44705883, 0.64705884, 0.8156863 ],\n",
       "          ...,\n",
       "          [0.15294118, 0.19607843, 0.28235295],\n",
       "          [0.15686275, 0.2       , 0.2784314 ],\n",
       "          [0.15686275, 0.2       , 0.2784314 ]],\n",
       " \n",
       "         [[0.4509804 , 0.654902  , 0.8156863 ],\n",
       "          [0.4509804 , 0.654902  , 0.8156863 ],\n",
       "          [0.4509804 , 0.654902  , 0.8156863 ],\n",
       "          ...,\n",
       "          [0.14901961, 0.19215687, 0.2784314 ],\n",
       "          [0.15294118, 0.19607843, 0.27450982],\n",
       "          [0.15686275, 0.2       , 0.2784314 ]],\n",
       " \n",
       "         [[0.4509804 , 0.654902  , 0.8156863 ],\n",
       "          [0.4509804 , 0.654902  , 0.8156863 ],\n",
       "          [0.45490196, 0.65882355, 0.81960785],\n",
       "          ...,\n",
       "          [0.14901961, 0.19215687, 0.2784314 ],\n",
       "          [0.15294118, 0.19607843, 0.27450982],\n",
       "          [0.15294118, 0.19607843, 0.27450982]]],\n",
       " \n",
       " \n",
       "        [[[0.45882353, 0.5529412 , 0.7176471 ],\n",
       "          [0.45490196, 0.54901963, 0.7137255 ],\n",
       "          [0.45490196, 0.54509807, 0.72156864],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.46666667, 0.56078434, 0.7254902 ],\n",
       "          [0.4627451 , 0.5568628 , 0.72156864],\n",
       "          [0.45882353, 0.54901963, 0.7254902 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.4627451 , 0.5529412 , 0.7294118 ],\n",
       "          [0.45882353, 0.54901963, 0.7254902 ],\n",
       "          [0.45490196, 0.54509807, 0.7254902 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.7529412 , 0.8745098 , 1.        ],\n",
       "          [0.7490196 , 0.87058824, 1.        ],\n",
       "          [0.74509805, 0.8666667 , 0.99607843],\n",
       "          ...,\n",
       "          [0.8       , 0.8901961 , 0.99215686],\n",
       "          [0.7921569 , 0.88235295, 0.9843137 ],\n",
       "          [0.7882353 , 0.8784314 , 0.98039216]],\n",
       " \n",
       "         [[0.7490196 , 0.87058824, 1.        ],\n",
       "          [0.74509805, 0.8666667 , 0.99607843],\n",
       "          [0.74509805, 0.8666667 , 0.99607843],\n",
       "          ...,\n",
       "          [0.8       , 0.8901961 , 0.99215686],\n",
       "          [0.7921569 , 0.88235295, 0.9843137 ],\n",
       "          [0.7882353 , 0.8784314 , 0.98039216]],\n",
       " \n",
       "         [[0.7411765 , 0.8627451 , 0.99215686],\n",
       "          [0.7411765 , 0.8627451 , 0.99215686],\n",
       "          [0.74509805, 0.8666667 , 0.99607843],\n",
       "          ...,\n",
       "          [0.8       , 0.8862745 , 0.99607843],\n",
       "          [0.7921569 , 0.8784314 , 0.9882353 ],\n",
       "          [0.7882353 , 0.8745098 , 0.9843137 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.77254903, 0.8666667 , 0.9764706 ],\n",
       "          [0.77254903, 0.8666667 , 0.9764706 ],\n",
       "          [0.7647059 , 0.87058824, 0.9764706 ],\n",
       "          ...,\n",
       "          [0.69411767, 0.8117647 , 0.9490196 ],\n",
       "          [0.6901961 , 0.80784315, 0.94509804],\n",
       "          [0.6901961 , 0.80784315, 0.94509804]],\n",
       " \n",
       "         [[0.7607843 , 0.85490197, 0.9647059 ],\n",
       "          [0.7647059 , 0.85882354, 0.96862745],\n",
       "          [0.77254903, 0.8666667 , 0.9764706 ],\n",
       "          ...,\n",
       "          [0.7019608 , 0.81960785, 0.95686275],\n",
       "          [0.7058824 , 0.8235294 , 0.9607843 ],\n",
       "          [0.7137255 , 0.83137256, 0.96862745]],\n",
       " \n",
       "         [[0.7529412 , 0.84705883, 0.95686275],\n",
       "          [0.7607843 , 0.85490197, 0.9647059 ],\n",
       "          [0.76862746, 0.8627451 , 0.972549  ],\n",
       "          ...,\n",
       "          [0.70980394, 0.827451  , 0.9647059 ],\n",
       "          [0.72156864, 0.8392157 , 0.9764706 ],\n",
       "          [0.7372549 , 0.85490197, 0.99215686]]]], dtype=float32),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]),\n",
       " array([[[[0.72156864, 0.8666667 , 0.8352941 ],\n",
       "          [0.6901961 , 0.8352941 , 0.8039216 ],\n",
       "          [0.80784315, 0.9411765 , 0.91764706],\n",
       "          ...,\n",
       "          [0.7176471 , 0.827451  , 0.85490197],\n",
       "          [0.7607843 , 0.8745098 , 0.89411765],\n",
       "          [0.7607843 , 0.8784314 , 0.8980392 ]],\n",
       " \n",
       "         [[0.7137255 , 0.84705883, 0.8235294 ],\n",
       "          [0.7372549 , 0.87058824, 0.84705883],\n",
       "          [0.8       , 0.9254902 , 0.90588236],\n",
       "          ...,\n",
       "          [0.80784315, 0.9098039 , 0.93333334],\n",
       "          [0.77254903, 0.8862745 , 0.90588236],\n",
       "          [0.7372549 , 0.8509804 , 0.87058824]],\n",
       " \n",
       "         [[0.73333335, 0.85490197, 0.84313726],\n",
       "          [0.6117647 , 0.73333335, 0.72156864],\n",
       "          [0.81960785, 0.93333334, 0.92156863],\n",
       "          ...,\n",
       "          [0.76862746, 0.8666667 , 0.88235295],\n",
       "          [0.8745098 , 0.972549  , 0.9882353 ],\n",
       "          [0.77254903, 0.8784314 , 0.89411765]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.74509805, 0.77254903, 0.8509804 ],\n",
       "          [0.78039217, 0.8156863 , 0.89411765],\n",
       "          [0.8392157 , 0.8745098 , 0.9529412 ],\n",
       "          ...,\n",
       "          [0.9372549 , 0.9529412 , 0.972549  ],\n",
       "          [0.8980392 , 0.90588236, 0.9490196 ],\n",
       "          [0.88235295, 0.88235295, 0.94509804]],\n",
       " \n",
       "         [[0.6901961 , 0.7137255 , 0.7882353 ],\n",
       "          [0.65882355, 0.6862745 , 0.7607843 ],\n",
       "          [0.68235296, 0.7176471 , 0.79607844],\n",
       "          ...,\n",
       "          [0.9137255 , 0.92941177, 0.9490196 ],\n",
       "          [0.87058824, 0.87058824, 0.91764706],\n",
       "          [0.9607843 , 0.9607843 , 1.        ]],\n",
       " \n",
       "         [[0.79607844, 0.8235294 , 0.8901961 ],\n",
       "          [0.74509805, 0.7764706 , 0.84313726],\n",
       "          [0.6039216 , 0.6313726 , 0.70980394],\n",
       "          ...,\n",
       "          [0.8980392 , 0.9137255 , 0.93333334],\n",
       "          [0.84313726, 0.84313726, 0.8901961 ],\n",
       "          [0.9843137 , 0.9843137 , 1.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.95686275, 0.972549  , 0.9529412 ],\n",
       "          [0.9647059 , 0.98039216, 0.9607843 ],\n",
       "          [0.972549  , 0.9882353 , 0.96862745],\n",
       "          ...,\n",
       "          [0.89411765, 0.91764706, 0.8509804 ],\n",
       "          [0.9254902 , 0.9490196 , 0.85882354],\n",
       "          [0.9137255 , 0.9372549 , 0.8392157 ]],\n",
       " \n",
       "         [[0.99215686, 1.        , 0.9882353 ],\n",
       "          [0.9882353 , 1.        , 0.9843137 ],\n",
       "          [0.99215686, 1.        , 0.9882353 ],\n",
       "          ...,\n",
       "          [0.972549  , 0.99215686, 0.93333334],\n",
       "          [0.9529412 , 0.96862745, 0.89411765],\n",
       "          [0.99215686, 1.        , 0.9254902 ]],\n",
       " \n",
       "         [[0.9843137 , 0.99607843, 0.9882353 ],\n",
       "          [0.95686275, 0.96862745, 0.9607843 ],\n",
       "          [0.99215686, 1.        , 0.99607843],\n",
       "          ...,\n",
       "          [0.972549  , 0.9882353 , 0.94509804],\n",
       "          [1.        , 1.        , 0.9607843 ],\n",
       "          [0.96862745, 0.98039216, 0.91764706]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.12941177, 0.20392157, 0.37254903],\n",
       "          [0.17254902, 0.25882354, 0.41960785],\n",
       "          [0.18039216, 0.28235295, 0.42745098],\n",
       "          ...,\n",
       "          [0.01960784, 0.0627451 , 0.21176471],\n",
       "          [0.07450981, 0.1254902 , 0.27450982],\n",
       "          [0.19607843, 0.24705882, 0.39607844]],\n",
       " \n",
       "         [[0.16078432, 0.28627452, 0.45490196],\n",
       "          [0.11372549, 0.23137255, 0.39215687],\n",
       "          [0.11372549, 0.22745098, 0.38039216],\n",
       "          ...,\n",
       "          [0.07058824, 0.10980392, 0.26666668],\n",
       "          [0.27450982, 0.3137255 , 0.47058824],\n",
       "          [0.30588236, 0.34509805, 0.5019608 ]],\n",
       " \n",
       "         [[0.        , 0.09803922, 0.2627451 ],\n",
       "          [0.10980392, 0.2509804 , 0.40784314],\n",
       "          [0.        , 0.10980392, 0.27058825],\n",
       "          ...,\n",
       "          [0.24705882, 0.28627452, 0.44313726],\n",
       "          [0.16078432, 0.2       , 0.35686275],\n",
       "          [0.21176471, 0.2509804 , 0.40784314]]],\n",
       " \n",
       " \n",
       "        [[[0.60784316, 0.62352943, 0.85490197],\n",
       "          [0.6       , 0.6156863 , 0.84705883],\n",
       "          [0.59607846, 0.6039216 , 0.85490197],\n",
       "          ...,\n",
       "          [0.36078432, 0.36078432, 0.65882355],\n",
       "          [0.34901962, 0.34901962, 0.64705884],\n",
       "          [0.3372549 , 0.3372549 , 0.63529414]],\n",
       " \n",
       "         [[0.6039216 , 0.61960787, 0.8509804 ],\n",
       "          [0.6       , 0.6117647 , 0.8509804 ],\n",
       "          [0.5921569 , 0.6       , 0.8509804 ],\n",
       "          ...,\n",
       "          [0.36862746, 0.36862746, 0.6666667 ],\n",
       "          [0.35686275, 0.35686275, 0.654902  ],\n",
       "          [0.34509805, 0.34509805, 0.6431373 ]],\n",
       " \n",
       "         [[0.6       , 0.6117647 , 0.8509804 ],\n",
       "          [0.59607846, 0.60784316, 0.84705883],\n",
       "          [0.5921569 , 0.6       , 0.8509804 ],\n",
       "          ...,\n",
       "          [0.3764706 , 0.3764706 , 0.6745098 ],\n",
       "          [0.3647059 , 0.3647059 , 0.6627451 ],\n",
       "          [0.35686275, 0.35686275, 0.654902  ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.5921569 , 0.7176471 , 1.        ],\n",
       "          [0.58431375, 0.70980394, 1.        ],\n",
       "          [0.5764706 , 0.69803923, 0.99215686],\n",
       "          ...,\n",
       "          [0.48235294, 0.5803922 , 0.9411765 ],\n",
       "          [0.4745098 , 0.57254905, 0.9411765 ],\n",
       "          [0.4627451 , 0.56078434, 0.92941177]],\n",
       " \n",
       "         [[0.59607846, 0.72156864, 1.        ],\n",
       "          [0.58431375, 0.70980394, 1.        ],\n",
       "          [0.57254905, 0.6901961 , 0.99215686],\n",
       "          ...,\n",
       "          [0.49019608, 0.5882353 , 0.9490196 ],\n",
       "          [0.48235294, 0.5803922 , 0.9411765 ],\n",
       "          [0.47058824, 0.5686275 , 0.92941177]],\n",
       " \n",
       "         [[0.6       , 0.7254902 , 1.        ],\n",
       "          [0.58431375, 0.70980394, 1.        ],\n",
       "          [0.57254905, 0.6901961 , 0.99215686],\n",
       "          ...,\n",
       "          [0.49803922, 0.59607846, 0.95686275],\n",
       "          [0.4862745 , 0.58431375, 0.94509804],\n",
       "          [0.47843137, 0.5764706 , 0.9372549 ]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.6156863 , 0.7019608 , 0.8117647 ],\n",
       "          [0.58431375, 0.67058825, 0.80784315],\n",
       "          [0.54509807, 0.62352943, 0.80784315],\n",
       "          ...,\n",
       "          [0.7137255 , 0.7411765 , 0.9372549 ],\n",
       "          [0.6784314 , 0.70980394, 0.91764706],\n",
       "          [0.6509804 , 0.69411767, 0.8980392 ]],\n",
       " \n",
       "         [[0.7411765 , 0.81960785, 0.95686275],\n",
       "          [0.6627451 , 0.7490196 , 0.8901961 ],\n",
       "          [0.6666667 , 0.7529412 , 0.91764706],\n",
       "          ...,\n",
       "          [0.627451  , 0.654902  , 0.8509804 ],\n",
       "          [0.69803923, 0.7411765 , 0.9372549 ],\n",
       "          [0.69803923, 0.7529412 , 0.94509804]],\n",
       " \n",
       "         [[0.68235296, 0.74509805, 0.9254902 ],\n",
       "          [0.65882355, 0.73333335, 0.9019608 ],\n",
       "          [0.6117647 , 0.70980394, 0.84313726],\n",
       "          ...,\n",
       "          [0.62352943, 0.6627451 , 0.8509804 ],\n",
       "          [0.6313726 , 0.6901961 , 0.87058824],\n",
       "          [0.5294118 , 0.5921569 , 0.77254903]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.7529412 , 0.7764706 , 0.8901961 ],\n",
       "          [0.61960787, 0.6509804 , 0.76862746],\n",
       "          [0.56078434, 0.59607846, 0.7294118 ],\n",
       "          ...,\n",
       "          [0.5803922 , 0.654902  , 0.8784314 ],\n",
       "          [0.49803922, 0.5686275 , 0.8       ],\n",
       "          [0.65882355, 0.7294118 , 0.9607843 ]],\n",
       " \n",
       "         [[0.64705884, 0.6627451 , 0.7764706 ],\n",
       "          [0.6313726 , 0.6509804 , 0.77254903],\n",
       "          [0.6666667 , 0.69803923, 0.84313726],\n",
       "          ...,\n",
       "          [0.52156866, 0.5882353 , 0.8117647 ],\n",
       "          [0.5764706 , 0.6392157 , 0.87058824],\n",
       "          [0.6666667 , 0.7294118 , 0.9607843 ]],\n",
       " \n",
       "         [[0.8666667 , 0.88235295, 0.99607843],\n",
       "          [0.7058824 , 0.7254902 , 0.84705883],\n",
       "          [0.6431373 , 0.6666667 , 0.8117647 ],\n",
       "          ...,\n",
       "          [0.47058824, 0.5372549 , 0.7607843 ],\n",
       "          [0.54509807, 0.60784316, 0.8392157 ],\n",
       "          [0.5882353 , 0.6509804 , 0.88235295]]],\n",
       " \n",
       " \n",
       "        [[[0.72156864, 0.80784315, 0.90588236],\n",
       "          [0.7294118 , 0.8156863 , 0.9137255 ],\n",
       "          [0.73333335, 0.81960785, 0.91764706],\n",
       "          ...,\n",
       "          [0.50980395, 0.627451  , 0.73333335],\n",
       "          [0.5019608 , 0.62352943, 0.74509805],\n",
       "          [0.4862745 , 0.60784316, 0.7294118 ]],\n",
       " \n",
       "         [[0.73333335, 0.81960785, 0.91764706],\n",
       "          [0.7372549 , 0.8235294 , 0.92156863],\n",
       "          [0.7411765 , 0.827451  , 0.9254902 ],\n",
       "          ...,\n",
       "          [0.5137255 , 0.6392157 , 0.7529412 ],\n",
       "          [0.5058824 , 0.6392157 , 0.75686276],\n",
       "          [0.49411765, 0.627451  , 0.74509805]],\n",
       " \n",
       "         [[0.7411765 , 0.827451  , 0.9254902 ],\n",
       "          [0.74509805, 0.83137256, 0.92941177],\n",
       "          [0.7490196 , 0.8352941 , 0.93333334],\n",
       "          ...,\n",
       "          [0.49803922, 0.6431373 , 0.7529412 ],\n",
       "          [0.4862745 , 0.63529414, 0.7529412 ],\n",
       "          [0.48235294, 0.6313726 , 0.75686276]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.69411767, 0.7607843 , 0.8627451 ],\n",
       "          [0.69411767, 0.7607843 , 0.8627451 ],\n",
       "          [0.69411767, 0.7607843 , 0.8627451 ],\n",
       "          ...,\n",
       "          [0.6784314 , 0.7372549 , 0.8392157 ],\n",
       "          [0.6745098 , 0.73333335, 0.8352941 ],\n",
       "          [0.6784314 , 0.7372549 , 0.8392157 ]],\n",
       " \n",
       "         [[0.6901961 , 0.75686276, 0.85882354],\n",
       "          [0.6901961 , 0.75686276, 0.85882354],\n",
       "          [0.69411767, 0.7607843 , 0.8627451 ],\n",
       "          ...,\n",
       "          [0.6745098 , 0.73333335, 0.8352941 ],\n",
       "          [0.67058825, 0.7294118 , 0.83137256],\n",
       "          [0.6745098 , 0.73333335, 0.8352941 ]],\n",
       " \n",
       "         [[0.6862745 , 0.7529412 , 0.85490197],\n",
       "          [0.6901961 , 0.75686276, 0.85882354],\n",
       "          [0.69411767, 0.7607843 , 0.8627451 ],\n",
       "          ...,\n",
       "          [0.67058825, 0.7294118 , 0.83137256],\n",
       "          [0.6666667 , 0.7254902 , 0.827451  ],\n",
       "          [0.6666667 , 0.7254902 , 0.827451  ]]],\n",
       " \n",
       " \n",
       "        [[[0.6901961 , 0.827451  , 0.92941177],\n",
       "          [0.6901961 , 0.827451  , 0.92941177],\n",
       "          [0.6901961 , 0.827451  , 0.92941177],\n",
       "          ...,\n",
       "          [0.40392157, 0.54509807, 0.7019608 ],\n",
       "          [0.40392157, 0.54901963, 0.69803923],\n",
       "          [0.40784314, 0.5529412 , 0.7019608 ]],\n",
       " \n",
       "         [[0.6901961 , 0.827451  , 0.92941177],\n",
       "          [0.6901961 , 0.827451  , 0.92941177],\n",
       "          [0.6901961 , 0.827451  , 0.92941177],\n",
       "          ...,\n",
       "          [0.39607844, 0.5372549 , 0.69411767],\n",
       "          [0.40392157, 0.54509807, 0.69411767],\n",
       "          [0.40392157, 0.54901963, 0.69803923]],\n",
       " \n",
       "         [[0.6901961 , 0.827451  , 0.92941177],\n",
       "          [0.6901961 , 0.827451  , 0.92941177],\n",
       "          [0.6901961 , 0.827451  , 0.92941177],\n",
       "          ...,\n",
       "          [0.39215687, 0.5294118 , 0.6862745 ],\n",
       "          [0.40392157, 0.53333336, 0.6862745 ],\n",
       "          [0.39607844, 0.5372549 , 0.6862745 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.67058825, 0.80784315, 0.9098039 ],\n",
       "          [0.67058825, 0.80784315, 0.9098039 ],\n",
       "          [0.67058825, 0.80784315, 0.9098039 ],\n",
       "          ...,\n",
       "          [0.45490196, 0.58431375, 0.6627451 ],\n",
       "          [0.4509804 , 0.5803922 , 0.65882355],\n",
       "          [0.4509804 , 0.5803922 , 0.65882355]],\n",
       " \n",
       "         [[0.6627451 , 0.8       , 0.9019608 ],\n",
       "          [0.6666667 , 0.8039216 , 0.90588236],\n",
       "          [0.6666667 , 0.8039216 , 0.90588236],\n",
       "          ...,\n",
       "          [0.45490196, 0.58431375, 0.6627451 ],\n",
       "          [0.4509804 , 0.5803922 , 0.65882355],\n",
       "          [0.4509804 , 0.5803922 , 0.65882355]],\n",
       " \n",
       "         [[0.65882355, 0.79607844, 0.8980392 ],\n",
       "          [0.6627451 , 0.8       , 0.9019608 ],\n",
       "          [0.6627451 , 0.8       , 0.9019608 ],\n",
       "          ...,\n",
       "          [0.45490196, 0.58431375, 0.6627451 ],\n",
       "          [0.4509804 , 0.5803922 , 0.65882355],\n",
       "          [0.4509804 , 0.5803922 , 0.65882355]]]], dtype=float32),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.datasets.cifar import load_batch\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "  data=[]\n",
    "  labels=[]\n",
    "  random.seed(42)\n",
    "  imagePaths = sorted(list(os.listdir(\"dataset\")))\n",
    "  random.shuffle(imagePaths)\n",
    "  print(imagePaths)\n",
    "\n",
    "  for img in imagePaths:#imagePaths=img=\n",
    "    path=sorted(list(os.listdir(\"dataset/\"+img)))#os.listdir\n",
    "    for i in path:#i=path=\n",
    "        image = cv2.imread(\"dataset/\"+img+'/'+i)\n",
    "        image = cv2.resize(image, (224,224))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)#data[]\n",
    "        l = label = img\n",
    "        labels.append(l)#labels\n",
    "  data = np.array(data, dtype=\"float32\") / 255.0\n",
    "  labels = np.array(labels)#\n",
    "  mlb = LabelBinarizer()#one-hot\n",
    "  labels = mlb.fit_transform(labels)\n",
    "  #print(labels[0])\n",
    "\n",
    "  (xtrain,xtest,ytrain,ytest)=train_test_split(data,labels,test_size=0.25,random_state=42)\n",
    "  #print(xtrain.shape, xtest.shape)\n",
    "  return xtrain, ytrain, xtest, ytest#xtrain=data,ytrain=lable\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa611292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Example to fine-tune on 3000 samples from Cifar10\n",
    "\n",
    "    img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "    channel = 3\n",
    "    num_classes =4\n",
    "    batch_size = 16 \n",
    "    nb_epoch = 10\n",
    "\n",
    "    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\n",
    "    X_train, Y_train, X_valid, Y_valid = load_data()\n",
    "\n",
    "    # Load our model\n",
    "    model = densenet121_model(img_rows=img_rows, img_cols=img_cols, color_type=channel, num_classes=num_classes)\n",
    "    filepath=\"bestmodel.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Start Fine-tuning\n",
    "    history=model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(X_valid, Y_valid),\n",
    "              callbacks=callbacks_list,\n",
    "              )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Cross-entropy loss score\n",
    "    score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Accuracy and Loss of the model\n",
    "%matplotlib inline\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) #No. of epochs\n",
    "\n",
    "#Plot training and validation accuracy per epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochs,acc,'b',label='Training Accuracy')\n",
    "plt.plot(epochs,val_acc,'r',label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "#Plot training and validation loss per epoch\n",
    "plt.plot(epochs,loss,'b',label='Training Loss')\n",
    "plt.plot(epochs,val_loss,'r',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "614dbc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total-test-data; 1 \taccurately-predicted-data: 0 \t wrongly-predicted-data:  1\n",
      "Accuracy: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "#X_valid, Y_valid\n",
    "xtest=X_valid\n",
    "ytest=Y_valid\n",
    "ypred = model.predict(xtest)\n",
    "\n",
    "total = 0\n",
    "accurate = 0\n",
    "accurateindex = []\n",
    "wrongindex = []\n",
    "\n",
    "for i in range(len(ypred)):\n",
    "    if np.argmax(ypred[i]) == np.argmax(ytest[i]):\n",
    "        accurate += 1\n",
    "        accurateindex.append(i)\n",
    "    else:\n",
    "        wrongindex.append(i)\n",
    "        \n",
    "    total += 1\n",
    "    \n",
    "print('Total-test-data;', total, '\\taccurately-predicted-data:', accurate, '\\t wrongly-predicted-data: ', total - accurate)\n",
    "print('Accuracy:', round(accurate/total*100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "168c8140",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-3c47a8f4450c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'airplane'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'car'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'flower'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'motorbike'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimidx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccurateindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# replace with 'wrongindex'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\desenet_2\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m        \u001b[1;31m# size of a small set minus size of an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "label=['airplane', 'car','flower','motorbike']\n",
    "imidx = random.sample(accurateindex, k=9)# replace with 'wrongindex'\n",
    "\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True,figsize=(15, 12))\n",
    "\n",
    "n = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "            ax[row,col].imshow(xtest[imidx[n]])\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(label[np.argmax(ypred[imidx[n]])], label[np.argmax(ytest[imidx[n]])]))\n",
    "            n += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66ccc9af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-a38e452fed13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'airplane'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'car'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'flower'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'motorbike'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimidx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrongindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# replace with 'wrongindex'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\desenet_2\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m        \u001b[1;31m# size of a small set minus size of an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "label=['airplane', 'car','flower','motorbike']\n",
    "imidx = random.sample(wrongindex, k=9)# replace with 'wrongindex'\n",
    "\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True,figsize=(15, 12))\n",
    "\n",
    "n = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "            ax[row,col].imshow(xtest[imidx[n]])\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(label[np.argmax(ypred[imidx[n]])], label[np.argmax(ytest[imidx[n]])]))\n",
    "            n += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ef9763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "\n",
    "\n",
    "#model=load_model('bestmodel.hdf5')\n",
    "\n",
    "#ypred = model.predict(xtest)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
